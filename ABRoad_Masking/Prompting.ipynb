{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMx9ZyJTaY6kVY9VIyO1bP1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8da0f2d92d104810b660a0ab8278f6bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dabfcd1c25b64eda94d53ca3e7ee16cc","IPY_MODEL_dc7e388487274b6d8cc71b904252e760","IPY_MODEL_c195321c9d49485f938ef7b905546f68"],"layout":"IPY_MODEL_fd153f789e7b44d999cbcf0e90a5678a"}},"dabfcd1c25b64eda94d53ca3e7ee16cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a94897aa2fd4f12a2bb169bb6204678","placeholder":"​","style":"IPY_MODEL_a8d535b4ec674de5ade7622792475611","value":"Downloading (…)lve/main/config.json: 100%"}},"dc7e388487274b6d8cc71b904252e760":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe3447678fff4a40af9e490b86266487","max":1110,"min":0,"orientation":"horizontal","style":"IPY_MODEL_decebd5266ae46b3a17ab0ffbd459d83","value":1110}},"c195321c9d49485f938ef7b905546f68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d031afaff084c30a24bae3299989e95","placeholder":"​","style":"IPY_MODEL_fd0596fa784e4281ac2bfeb1d8954002","value":" 1.11k/1.11k [00:00&lt;00:00, 10.7kB/s]"}},"fd153f789e7b44d999cbcf0e90a5678a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a94897aa2fd4f12a2bb169bb6204678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d535b4ec674de5ade7622792475611":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe3447678fff4a40af9e490b86266487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"decebd5266ae46b3a17ab0ffbd459d83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d031afaff084c30a24bae3299989e95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd0596fa784e4281ac2bfeb1d8954002":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52ebba1cf1c2432ea1caf454a53184dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2e18326f1d149bfacb4bf5f3eb53300","IPY_MODEL_d7e8f51e48a140678f2be3b51e162cf0","IPY_MODEL_d59e63a5db64423dade3ccd6599a3f83"],"layout":"IPY_MODEL_5f9353fdc9644f69be46f6dc24d074ac"}},"a2e18326f1d149bfacb4bf5f3eb53300":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f74cd34c036a4e2fb87abb4e53fe339c","placeholder":"​","style":"IPY_MODEL_726e53fe5a9f496a9ec60aff118ebbd2","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"d7e8f51e48a140678f2be3b51e162cf0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_032e521b19f54a5287447cad483cf865","max":435783451,"min":0,"orientation":"horizontal","style":"IPY_MODEL_998c165c6aba4e3090c76fe6c4dfa74c","value":435783451}},"d59e63a5db64423dade3ccd6599a3f83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1c4a69cb65046ab904aa4eff60d4d00","placeholder":"​","style":"IPY_MODEL_af5f5d135529425e8246b162cc3a30ae","value":" 436M/436M [00:05&lt;00:00, 82.0MB/s]"}},"5f9353fdc9644f69be46f6dc24d074ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74cd34c036a4e2fb87abb4e53fe339c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"726e53fe5a9f496a9ec60aff118ebbd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"032e521b19f54a5287447cad483cf865":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"998c165c6aba4e3090c76fe6c4dfa74c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1c4a69cb65046ab904aa4eff60d4d00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af5f5d135529425e8246b162cc3a30ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef9ea832121646f0a2746c88ec114d59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_050a9b4fd1624abbbff7b98220fcb8a5","IPY_MODEL_57c482726fda44adad0e1bb0e0c6fead","IPY_MODEL_a31f0f41ebe549a4aa7fbbfc5194d699"],"layout":"IPY_MODEL_f8ce42a6da3d4af988880b6f9b0d4296"}},"050a9b4fd1624abbbff7b98220fcb8a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_619b162cb1be4f9baa281c531a0d441c","placeholder":"​","style":"IPY_MODEL_d2eadc65b2f949b4af904934aa5a1b30","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"57c482726fda44adad0e1bb0e0c6fead":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a23c186f75fa48e7b8c042483cbef847","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_469f43ff35ce414facf6561f45e4d99b","value":213450}},"a31f0f41ebe549a4aa7fbbfc5194d699":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d51a0ea8efd4ded96ea97e086ac1245","placeholder":"​","style":"IPY_MODEL_ad5fd906f0044e66b957af3ee89e0b95","value":" 213k/213k [00:00&lt;00:00, 1.54MB/s]"}},"f8ce42a6da3d4af988880b6f9b0d4296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"619b162cb1be4f9baa281c531a0d441c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2eadc65b2f949b4af904934aa5a1b30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a23c186f75fa48e7b8c042483cbef847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"469f43ff35ce414facf6561f45e4d99b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d51a0ea8efd4ded96ea97e086ac1245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad5fd906f0044e66b957af3ee89e0b95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Prompting strategies"],"metadata":{"id":"KFpo-JT3ddww"}},{"cell_type":"code","source":["# install libraries\n","!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVkcROHOddYy","executionInfo":{"status":"ok","timestamp":1675175374755,"user_tz":-60,"elapsed":16333,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"c09fa78d-7d97-422c-d40a-748439146db5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUquXnAQcf9N","executionInfo":{"status":"ok","timestamp":1675175394279,"user_tz":-60,"elapsed":19529,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"b35ce85c-06c6-4659-ca2a-433389e8b57b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/data\n"]}],"source":["# Google Colab: set current dir\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/data"]},{"cell_type":"code","source":["# load libraries\n","import os\n","import gc\n","import transformers\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from transformers.file_utils import is_torch_available\n","from scipy.spatial import distance\n","from scipy.stats import pearsonr"],"metadata":{"id":"4lClWR9_dpzA","executionInfo":{"status":"ok","timestamp":1675184623142,"user_tz":-60,"elapsed":252,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":144,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-G8HgD9VQ0a8","executionInfo":{"status":"ok","timestamp":1675175407233,"user_tz":-60,"elapsed":22,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"9f80ab28-0733-444d-cc4c-39ce7966c0ba"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU available, using the CPU instead.\n"]}]},{"cell_type":"code","source":["def set_seed(seed: int):\n","    \"\"\"\n","    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n","    installed).\n","\n","    Args:\n","        seed (:obj:`int`): The seed to set.\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    if is_torch_available():\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(1024)"],"metadata":{"id":"-OuE-wr9DqKh","executionInfo":{"status":"ok","timestamp":1675175407233,"user_tz":-60,"elapsed":18,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# load data\n","dir = \"prompting-data\"\n","data_file = \"dataset_shorter_chem_names.tsv\"\n","\n","data = pd.read_csv(os.path.join(dir, data_file), sep=\"\\t\")\n","print(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YDGj-n0dycX","executionInfo":{"status":"ok","timestamp":1675184626188,"user_tz":-60,"elapsed":248,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"24c24984-fcb7-4bd5-dffb-aa12f2537ce9"},"execution_count":145,"outputs":[{"output_type":"stream","name":"stdout","text":["   fungi_id                 fungi_name       family_name  pubchem_id  \\\n","0    119834       Alternaria alternata     Pleosporaceae     5360741   \n","1    257047  Cephalosporium aphidicola   Cordycipitaceae      457964   \n","2    237604        Cordyceps militaris   Cordycipitaceae        6303   \n","3    284309          Aspergillus niger    Aspergillaceae     5748546   \n","4    815927     Albifimbria verrucaria  Stachybotryaceae     6326658   \n","\n","                      chem_name  nb_ref  y  \n","0  Alternariol monomethyl ether      11  1  \n","1                   Aphidicolin      10  1  \n","2                    Cordycepin       6  1  \n","3                  Flavasperone       6  1  \n","4                  Verrucarin A       5  1  \n"]}]},{"cell_type":"code","source":["models = {'ChemicalBERT':'recobo/chemical-bert-uncased',\n","\n","          'BioBERT':'dmis-lab/biobert-base-cased-v1.2',\n","\n","          'BERT':'bert-base-uncased',\n","          'BERT-large': 'bert-large-cased-whole-word-masking',\n","\n","          'RoBERTa':'roberta-base', # needs <mask>\n","          'RoBERTa-large':'roberta-large',\n","\n","          'BigBird-RoBERTa-large':'google/bigbird-roberta-large',\n","\n","          'Muppet-RoBERTa-large':'facebook/muppet-roberta-large',\n","          \n","          'PubMedBERT-full':'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n","          'PubMedBERT':'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n","          \n","          'Clinical-BigBird':'yikuan8/Clinical-BigBird',\n","          'Clinical-Longformer':'yikuan8/Clinical-Longformer'\n","}"],"metadata":{"id":"y5g3rI3sfSqX","executionInfo":{"status":"ok","timestamp":1675175407680,"user_tz":-60,"elapsed":6,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Task 1: Association Fungi - Chemical compound"],"metadata":{"id":"3gTMxYoLhjXv"}},{"cell_type":"code","source":["# 1) Set-up manual prompts verbaliser\n","\n","class ManualPromptDataset(Dataset):\n","  \"\"\"\n","  Dataset generator for task1.\n","  Generate manual prompt for each pairs in the dataset.\n","  Parameters are:\n","    - the dataset\n","    - the model to select the AutoTokenizer\n","    - a prompt template\n","  \"\"\"\n","\n","  def __init__(self, data, tokenizer, template, max_length):\n","    self.data = data\n","    self.tokenizer = tokenizer\n","    self.template = template\n","    self.max_length = max_length\n","  \n","  def __len__(self):\n","    return self.data.shape[0]\n","  \n","  def __getitem__(self, index):\n","\n","    # Get data\n","    chemical_name = self.data.loc[index, \"chem_name\"]\n","    fungi_name = self.data.loc[index, \"fungi_name\"]\n","    # fill and tokenize prompt template\n","    filled_template = self.template.format(compound=chemical_name, mask=self.tokenizer.mask_token, fungi=fungi_name)\n","    tokenized = self.tokenizer(filled_template, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n","    \n","    for k in tokenized.keys():\n","      tokenized[k] = torch.squeeze(tokenized[k])\n","\n","    # remove \n","    return tokenized\n","\n","\n","def get_proba_matrix(dataloader, model, vocab_size, mask_token_id, device):\n","  \"\"\"\n","  Evaluate the performances of each models and each template \n","  \"\"\"\n","\n","  # Init token expected counts\n","  proba_matrix = np.array([]).reshape(0, vocab_size)\n","\n","  for step, batch in enumerate(dataloader):\n","    print(\"    - batch: \" + str(step))\n","\n","    n, m = batch[\"input_ids\"].shape\n","\n","    # save input ids in classic device before using gpu (in case)\n","    input_ids = batch[\"input_ids\"].clone()\n","    \n","    inputs = batch.to(device)\n","    \n","    # send batch to model\n","    out = model(**inputs)\n","\n","    # get proba for the masked token\n","    # 1- recover indexes of the masked token\n","    masked_token_indexes = (input_ids == mask_token_id).nonzero(as_tuple=True)[1]\n","\n","    # 2- transform indexes so that we can extract the correspond line in the 3D tensor. The idea is to transform the 3D tensor (batch_size, seq_length, hidden_size) en un 2D tensor (batch_size * seq_length, hidden_size).\n","    # Ensuite, on a plus qu'a incrémenter les index initiaux de 64 en plus pour chaque ligne de tel sorte à ce qu'il corresponde dans la matrice 2D.\n","    masked_token_indexes = masked_token_indexes + torch.arange(0, m * n, m)\n","\n","    # Compute proba\n","    proba_masked_tokens = torch.nn.functional.softmax(out.logits[:, :, 0:vocab_size].view(-1, vocab_size)[masked_token_indexes], dim=1)\n","\n","    # concat in proba matrix\n","    proba_matrix = np.concatenate((proba_matrix, proba_masked_tokens.detach().cpu().numpy()), axis=0)\n","\n","  return proba_matrix\n","\n","\n","\n","def get_expected_top_k(dataloader, model, vocab_size, mask_token_id, k, device):\n","  \"\"\"\n","  Get the top k expected tokens\n","  \"\"\"\n","\n","  expected_count_matrix = np.zeros(vocab_size)\n","\n","  for step, batch in enumerate(dataloader):\n","    print(\"    - batch: \" + str(step))\n","\n","    n, m = batch[\"input_ids\"].shape\n","\n","    # save input ids in classic device before using gpu (in case)\n","    input_ids = batch[\"input_ids\"].clone()\n","    \n","    inputs = batch.to(device)\n","    \n","    # send batch to model\n","    out = model(**inputs)\n","\n","    # get proba for the masked token\n","    # 1- recover indexes of the masked token\n","    masked_token_indexes = (input_ids == mask_token_id).nonzero(as_tuple=True)[1]\n","\n","    # 2- transform indexes so that we can extract the correspond line in the 3D tensor. The idea is to transform the 3D tensor (batch_size, seq_length, hidden_size) en un 2D tensor (batch_size * seq_length, hidden_size).\n","    # Ensuite, on a plus qu'a incrémenter les index initiaux de 64 en plus pour chaque ligne de tel sorte à ce qu'il corresponde dans la matrice 2D.\n","    masked_token_indexes = masked_token_indexes + torch.arange(0, m * n, m)\n","\n","    # Compute proba\n","    proba_masked_tokens = torch.nn.functional.softmax(out.logits[:, :, 0:vocab_size].view(-1, vocab_size)[masked_token_indexes], dim=1)\n","\n","    # concat in proba matrix\n","    expected_count_matrix += np.sum(proba_masked_tokens.detach().cpu().numpy(), axis=0)\n","  \n","  top_k_indexes = np.argsort(expected_count_matrix)[::-1][:k]\n","  top_k_values = expected_count_matrix[top_k_indexes]\n","\n","  return top_k_indexes, top_k_values\n","\n","\n","  \n","def get_top_1_distribution(input_matrix):\n","  \n","  # init\n","  n, m = input_matrix.shape\n","  counts = np.zeros(m)\n","  \n","  # get top 1 for each example\n","  for i in range(n):\n","    top1_index = np.argmax(input_matrix[i])\n","    counts[top1_index] += 1\n","  \n","  # transform as probs\n","  counts = counts / np.sum(counts)\n","  return counts\n","\n","\n","\n","\n","def compute_JS_divergences(m1, m2, n_sample):\n","  \"\"\"\n","  m1 and m2 are matrix of words probability od dim (n x V) where n in the numner of samples and V the vocabulary size.\n","  \"\"\"\n","  \n","  if not m1.shape == m2.shape:\n","    print(\"m1 and n2 must have the same dimensions\")\n","    return False\n","  \n","  n, m = m1.shape\n","  JS_divergences = np.empty(n_sample)\n","  \n","  for k in range(n_sample):\n","\n","    i = random.choice(range(n))\n","    j = random.choice(range(n))\n","\n","    JS_divergences[k] = distance.jensenshannon(m1[i], m2[j])\n","  \n","  return JS_divergences"],"metadata":{"id":"fYzJ475xhnlh","executionInfo":{"status":"ok","timestamp":1675175407681,"user_tz":-60,"elapsed":6,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Les templates\n","\n","-  On a des templates pour faire de la sentiment analysis: on demande de compléter par un verbe ou un adjectif qui devrait être représentatif de la nature du statement : vrai ou faux\n","\n","-  Les templates dits de prédiction demande quant à eux de compléter la phrase avec un composé chimique."],"metadata":{"id":"8br8zWld-MM4"}},{"cell_type":"code","source":["templates_task1_sentiment = ['Compound {compound} was {mask} from fungus {fungi} with the antimicrobial guided isolation procedure.',\n","                 '{compound} was {mask} from {fungi} with the antimicrobial guided isolation procedure.',\n","                 'Compound {compound} was {mask} obtained from fungus {fungi} with the antimicrobial guided isolation procedure.',\n","                 '{compound} was {mask} obtained from {fungi} with the antimicrobial guided isolation procedure.',          \n","                 'Fungus {fungi} showed {mask} {compound} activity.',\n","                 '{fungi} showed {mask} {compound} activity.',\n","                 'Fungus {fungi} {mask} {compound} activity.',\n","                 '{fungi} {mask} {compound} activity.',\n","                 'Fungus {fungi} {mask} compound {compound}.',\n","                 'Authors {mask} a natural product called {compound} from the {fungi}.',\n","                 'A strain {fungi} was isolated as a {mask} {compound} producer.',                 \n","                 'A strain {fungi} was {mask} as a high {compound} producer.',\n","                 'Compound {compound} is produced by fungus {fungi}. It is {mask}.']\n","\n","template_task1_prediction = ['The fungus {fungi} is a natural producer of the compound {mask}.',\n","                             'The compound {mask} was isolated and identified from culture of fungus {fungi}',\n","                             'Seconday metabolite {compound} has been isolated from crude extracts of fungi {fungi}.']\n","\n","templates_task2_sentiment = ['Among isolated chemical compounds, {compound} presented {mask} antimicrobial activities.',\n","                 'Compound {compound} showed {mask} growth inhibition on strains.',\n","                 '{compound} showed {mask} growth inhibition on strains.',\n","                 'Compound {compound} showed {mask} growth inhibition on drug-resistant pathogenic strains.',\n","                 '{compound} showed {mask} growth inhibition on drug-resistant pathogenic strains.',\n","                 'Compound {compound} {mask} the growth of the strains.',\n","                 '{compound} {mask} the growth of the strains.',\n","                 'Compound {compound} has antibiotic activity. It is {mask}']\n","\n"," \n","templates_task2_prediction = ['Compound {chemical} showed {mask} activity.']\n","\n","new_templates_task2_sentiment = ['Compound {compound} has antibiotic activity. It is {mask}']\n","\n","\n","test_models = {'BioBERT':'dmis-lab/biobert-base-cased-v1.2',\n","               'RoBERTa':'roberta-base'}\n","\n","test_templates_task1_sentiment = ['Compound {compound} was {mask} from fungus {fungi} with the antimicrobial guided isolation procedure.',\n","  '{compound} was {mask} from {fungi} with the antimicrobial guided isolation procedure.']"],"metadata":{"id":"KXYKb3WNSxVW","executionInfo":{"status":"ok","timestamp":1675175407682,"user_tz":-60,"elapsed":6,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["positive_pairs = data[data[\"y\"] == 1].reset_index()\n","negative_pairs = data[data[\"y\"] == 0].reset_index()\n","\n","model = AutoModelForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n","model.to(device)\n","tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', use_fast=True)\n","vocab_size = tokenizer.vocab_size\n","mask_token_id = tokenizer.mask_token_id\n","\n","template = templates_task2_sentiment[0]\n","max_length = 64\n","batch_size = 64\n","\n","dataset = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","dataloader_positive = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","# a = get_proba_matrix(dataloader_positive, model, vocab_size, mask_token_id, device)"],"metadata":{"id":"BcdXkFP5FQnf","colab":{"base_uri":"https://localhost:8080/","height":185,"referenced_widgets":["8da0f2d92d104810b660a0ab8278f6bf","dabfcd1c25b64eda94d53ca3e7ee16cc","dc7e388487274b6d8cc71b904252e760","c195321c9d49485f938ef7b905546f68","fd153f789e7b44d999cbcf0e90a5678a","2a94897aa2fd4f12a2bb169bb6204678","a8d535b4ec674de5ade7622792475611","fe3447678fff4a40af9e490b86266487","decebd5266ae46b3a17ab0ffbd459d83","3d031afaff084c30a24bae3299989e95","fd0596fa784e4281ac2bfeb1d8954002","52ebba1cf1c2432ea1caf454a53184dc","a2e18326f1d149bfacb4bf5f3eb53300","d7e8f51e48a140678f2be3b51e162cf0","d59e63a5db64423dade3ccd6599a3f83","5f9353fdc9644f69be46f6dc24d074ac","f74cd34c036a4e2fb87abb4e53fe339c","726e53fe5a9f496a9ec60aff118ebbd2","032e521b19f54a5287447cad483cf865","998c165c6aba4e3090c76fe6c4dfa74c","c1c4a69cb65046ab904aa4eff60d4d00","af5f5d135529425e8246b162cc3a30ae","ef9ea832121646f0a2746c88ec114d59","050a9b4fd1624abbbff7b98220fcb8a5","57c482726fda44adad0e1bb0e0c6fead","a31f0f41ebe549a4aa7fbbfc5194d699","f8ce42a6da3d4af988880b6f9b0d4296","619b162cb1be4f9baa281c531a0d441c","d2eadc65b2f949b4af904934aa5a1b30","a23c186f75fa48e7b8c042483cbef847","469f43ff35ce414facf6561f45e4d99b","9d51a0ea8efd4ded96ea97e086ac1245","ad5fd906f0044e66b957af3ee89e0b95"]},"executionInfo":{"status":"ok","timestamp":1675175420453,"user_tz":-60,"elapsed":12516,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"0457769b-6ee1-41ca-ceb6-ee056ac550a7"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da0f2d92d104810b660a0ab8278f6bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ebba1cf1c2432ea1caf454a53184dc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9ea832121646f0a2746c88ec114d59"}},"metadata":{}}]},{"cell_type":"code","source":["model = AutoModelForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n","tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', use_fast=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbe3VHXgZXdQ","executionInfo":{"status":"ok","timestamp":1675181739915,"user_tz":-60,"elapsed":4045,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"536dca5c-0681-46db-8477-39110a0153b7"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["input = tokenizer(\"carcinosarcoma of lung has a genetic association with [MASK] [MASK] [MASK] [MASK]\", max_length=64, padding='max_length', truncation=True, return_tensors='pt')"],"metadata":{"id":"aDO6E_mMm0-L","executionInfo":{"status":"ok","timestamp":1675182578542,"user_tz":-60,"elapsed":227,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["(input[\"input_ids\"] == mask_token_id).nonzero(as_tuple=True)[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgnVfdb2nIIa","executionInfo":{"status":"ok","timestamp":1675182579812,"user_tz":-60,"elapsed":242,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"28f15073-ee59-4911-f5d7-6be264f2967c"},"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([13, 14, 15, 16])"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["tokenizer.encode(\"deoxy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvF4YK3Enyki","executionInfo":{"status":"ok","timestamp":1675183393236,"user_tz":-60,"elapsed":359,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"a89f268d-81ae-4164-8dc9-d24dea5a241e"},"execution_count":140,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 1260, 10649, 1183, 102]"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["tokenizer.decode(1183)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bJAllDX4thxi","executionInfo":{"status":"ok","timestamp":1675183417333,"user_tz":-60,"elapsed":6,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"686b2aa9-8bdf-4a5f-c627-b09ca2c2236f"},"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'##y'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["out = model(**input)"],"metadata":{"id":"PHXiC_fDArFd","executionInfo":{"status":"ok","timestamp":1675182586672,"user_tz":-60,"elapsed":877,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["[np.argmax(out.logits[0, i, :].cpu().detach().numpy()) for i in [13, 14, 15, 16]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQ66-hQhhltE","executionInfo":{"status":"ok","timestamp":1675182587791,"user_tz":-60,"elapsed":3,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"67aa4fee-c85d-4b9b-898b-15d8a93d72d0"},"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[145, 1105, 3187, 119]"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["print(tokenizer.decode(5358))\n","print(tokenizer.decode(8508))\n","print(tokenizer.decode(2042))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSRVCaJob0WN","executionInfo":{"status":"ok","timestamp":1675182279664,"user_tz":-60,"elapsed":205,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"54a1dd51-6750-4484-d7fc-39e7481b30df"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["da\n","##pi\n","##ine\n"]}]},{"cell_type":"code","source":["tokenizer.decode([101,  1610, 16430,  9275, 19878,  7903,  1104, 13093,  1144,   170,\n","          7434,  3852,  1114,   145, 1105, 3187, 119,   102])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CRXXk6IHpZ4h","executionInfo":{"status":"ok","timestamp":1675182640869,"user_tz":-60,"elapsed":334,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"a7713742-897b-4625-fb60-cf383a958ff8"},"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] carcinosarcoma of lung has a genetic association with H and risk. [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":134}]},{"cell_type":"markdown","source":["## Run evaluation on all combinations and save the results\n","\n","### What's the idea ?\n","\n","-- Hyp: The distribution of the answers should be different between true pairs and negative pairs, and this difference should be sufficient significant.\n","\n","-- How to compute the difference betwwen the answers distribution ? We compute the JS-divergence between the probability distribution of words for the MASKED token.\n","\n","So, the JS-divergence observed for the word probability distribution obtained between a positive pair and a negative pair should be high. The model should not answer the same thing in the both case.\n","\n","-- How to tell if it is significant ?\n","To estimate if the JS-Divergence value is signficant, we can compare it to JS-Divergence values obtained between pairs of positive or pairs of negative examples. Between pairs of positive examples (or between pairs of negative examples) the JS divergence should be small as the model should have a similar answer distribution whem it's positive examples or when its negative examples.\n","\n","So the idea is simply to do like a sort of Monte-Carlo p.value, we estimate the probability that the JS-Divergence between Pos and Neg > the JS-Divergence between Pos examples OR JS-Divergence between Neg examples.\n","\n","\n","If we found close to 0.5 (like we did), this means that the answer distribution by comparing Positive and Negatives is not much diffenrent than the natural variability we observed by comparing postives examples or negative examples together. \n","\n","\n","\n","\n"],"metadata":{"id":"RlEFz8cYLARm"}},{"cell_type":"code","source":["\n","def compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models_set, templates_set, n_sample):\n","\n","  proba_matrix_positives = np.array([])\n","  proba_matrix_negatives = np.array([])\n","\n","  print(\"Device: \" + str(device))\n","  batch_size = 64\n","  max_length = 64\n","  dir = \"prompting-data/results\"\n","\n","  JS_divergence_table = pd.DataFrame()\n","\n","  # For each model\n","  for model_name, model_ref in models_set.items():\n","    \n","    print(\"Treating model \" + model_name)\n","\n","    # load model and tokenizer\n","    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n","    model.to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n","    vocab_size = tokenizer.vocab_size\n","    mask_token_id = tokenizer.mask_token_id\n","\n","    # just inference, no backward needed\n","    with torch.no_grad(): \n","\n","      for template_index in range(len(templates_set)):\n","\n","        print(\" - Template: \" + str(template_index))\n","        \n","        template = templates_set[template_index]\n","\n","        # load data for POSTIVE pairs\n","        dataset = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_positive = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get proba matrix for positive examples\n","        proba_matrix_positives = get_proba_matrix(dataloader_positive, model, vocab_size, mask_token_id, device)\n","\n","        # load data for NEGATIVE pairs\n","        dataset = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_negative = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get proba matrix for negative examples\n","        proba_matrix_negatives = get_proba_matrix(dataloader_negative, model, vocab_size, mask_token_id, device)\n","\n","        # Compute Positive x Positive JS divergence:\n","        pos_pos_JS_divergences = compute_JS_divergences(proba_matrix_positives, proba_matrix_positives, n_sample)\n","\n","        # Compute Negative x Negative JS divergence:\n","        neg_neg_JS_divergences = compute_JS_divergences(proba_matrix_negatives, proba_matrix_negatives, n_sample)\n","\n","        # Compute Postive x Negative JS divergence:\n","        pos_neg_JS_divergences = compute_JS_divergences(proba_matrix_positives, proba_matrix_negatives, n_sample)\n","        n = len(pos_neg_JS_divergences)\n","\n","        # Compile and export\n","        model_template_JS_table = pd.DataFrame({\"model\": [model_name] * n, \"Template\": [template_index] * n, \"PosxPos\": pos_pos_JS_divergences, \"NegxNeg\": neg_neg_JS_divergences, \"PosxNeg\": pos_neg_JS_divergences})\n","        JS_divergence_table = pd.concat([JS_divergence_table, model_template_JS_table])\n","\n","    # clean\n","    model = None\n","    tokenizer = None\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","  \n","  # output\n","  return JS_divergence_table\n","\n","\n","\n","def compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models_set, templates_set, k):\n","\n","  print(\"Device: \" + str(device))\n","  batch_size = 64\n","  max_length = 64\n","\n","  top_k_table = pd.DataFrame()\n","\n","  # For each model\n","  for model_name, model_ref in models_set.items():\n","    \n","    print(\"Treating model \" + model_name)\n","\n","    # load model and tokenizer\n","    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n","    model.to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n","    vocab_size = tokenizer.vocab_size\n","    mask_token_id = tokenizer.mask_token_id\n","\n","    # just inference, no backward needed\n","    with torch.no_grad(): \n","\n","      for template_index in range(len(templates_set)):\n","\n","        print(\" - Template: \" + str(template_index))\n","        \n","        template = templates_set[template_index]\n","\n","        # load data for POSTIVE pairs\n","        dataset_positive = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_positive = DataLoader(dataset_positive, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get tokens expected counts for positive examples\n","        top_k_indexes_positives, top_k_e_counts_positives = get_expected_top_k(dataloader_positive, model, vocab_size, mask_token_id, k, device)\n","        top_k_tokens_positives = [tokenizer.decode(t) for t in top_k_indexes_positives]\n","\n","        # load data for NEGATIVE pairs\n","        dataset_negative = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_negative = DataLoader(dataset_negative, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get tokens expected counts for negative examples\n","        top_k_indexes_negatives, top_k_e_counts_negatives = get_expected_top_k(dataloader_negative, model, vocab_size, mask_token_id, k, device)\n","        top_k_tokens_negatives = [tokenizer.decode(t) for t in top_k_indexes_negatives]\n","\n","        # Compile and export\n","        n = k * 2\n","        model_template_top_k_table = pd.DataFrame({\"model\": [model_name] * n, \"Template\": [template_index] * n, \"Type\": [\"Positive\"] * k + [\"Negative\"] * k, \"Rank\": list(range(1, k + 1)) * 2, \"index\": np.concatenate((top_k_indexes_positives, top_k_indexes_negatives)), \"word\": np.concatenate((top_k_tokens_positives, top_k_tokens_negatives)), \"count\": np.concatenate((top_k_e_counts_positives, top_k_e_counts_negatives))})\n","        top_k_table = pd.concat([top_k_table, model_template_top_k_table])\n","\n","    # clean\n","    model = None\n","    tokenizer = None\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","  return top_k_table\n","\n","\n","\n","def compute_top1_correlation(positive_pairs, negative_pairs, models_set, templates_set):\n","\n","  print(\"Device: \" + str(device))\n","  batch_size = 64\n","  max_length = 64\n","\n","  top1_correlations = pd.DataFrame()\n","  n_templates = len(templates_set)\n","\n","  # For each model\n","  for model_name, model_ref in models_set.items():\n","    \n","    print(\"Treating model \" + model_name)\n","\n","    # load model and tokenizer\n","    model = AutoModelForMaskedLM.from_pretrained(model_ref)\n","    model.to(device)\n","    tokenizer = AutoTokenizer.from_pretrained(model_ref, use_fast=True)\n","    vocab_size = tokenizer.vocab_size\n","    mask_token_id = tokenizer.mask_token_id\n","\n","    cors = np.empty(n_templates)\n","\n","    # just inference, no backward needed\n","    with torch.no_grad(): \n","\n","      for template_index in range(len(templates_set)):\n","\n","        print(\" - Template: \" + str(template_index))\n","        \n","        template = templates_set[template_index]\n","\n","        # load data for POSTIVE pairs\n","        dataset_positive = ManualPromptDataset(data=positive_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_positive = DataLoader(dataset_positive, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get proba matrix for positive examples\n","        proba_matrix_positives = get_proba_matrix(dataloader_positive, model, vocab_size, mask_token_id, device)\n","\n","        # load data for NEGATIVE pairs\n","        dataset_negative = ManualPromptDataset(data=negative_pairs, tokenizer=tokenizer, template=template, max_length=max_length)\n","        dataloader_negative = DataLoader(dataset_negative, batch_size = batch_size, shuffle = False, num_workers = 0)\n","\n","        # get proba matrix for negative examples\n","        proba_matrix_negatives = get_proba_matrix(dataloader_negative, model, vocab_size, mask_token_id, device)\n","\n","        # Cpmpute top 1 correlations\n","        top1_pos = get_top_1_distribution(proba_matrix_positives)\n","        top1_neg = get_top_1_distribution(proba_matrix_negatives)\n","        cors[template_index] = pearsonr(top1_pos, top1_neg)[0]\n","\n","    # Compile and export\n","    model_template_cors = pd.DataFrame({\"model\": [model_name] * n_templates , \"Template\": list(range(n_templates)), \"cors\": cors})\n","    top1_correlations = pd.concat([top1_correlations, model_template_cors])\n","\n","    # clean\n","    model = None\n","    tokenizer = None\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","  \n","  # output\n","  return top1_correlations\n"],"metadata":{"id":"NBOC0gutK_0k","executionInfo":{"status":"ok","timestamp":1675171823211,"user_tz":-60,"elapsed":197,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#### For task 1:"],"metadata":{"id":"9jrYtHrgGJQU"}},{"cell_type":"code","source":["##  Get JS - Divergences\n","n_sample = 5000\n","positive_pairs = data[data[\"nb_ref\"] > 0].reset_index()\n","negative_pairs = data[data[\"nb_ref\"] < 0].reset_index()\n","\n","JS = compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models, templates_task1_sentiment, n_sample)\n","JS.to_csv(\"prompting-data/results/JS_divergence_complete_template_task1_5000.tsv\", index=False, sep=\"\\t\")\n","\n","##  Get Top k tokens\n","k = 20\n","TOP_K = compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models, templates_task1_sentiment, k)\n","TOP_K.to_csv(\"prompting-data/results/top_k_table_task1.tsv\", index=False, sep=\"\\t\")"],"metadata":{"id":"SnRCGtEcETJ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the Top 1 token correlation\n","\n","positive_pairs = data[data[\"nb_ref\"] > 0].reset_index()\n","negative_pairs = data[data[\"nb_ref\"] < 0].reset_index()\n","\n","TOP_1_COR = compute_top1_correlation(positive_pairs, negative_pairs, models, templates_task1_sentiment)\n","TOP_1_COR.to_csv(\"prompting-data/results/top_1_cor_task1.tsv\", index=False, sep=\"\\t\")"],"metadata":{"id":"0eGVbwXKa3kn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TOP_1_COR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"ZT7q3IKKcREk","executionInfo":{"status":"ok","timestamp":1675162391600,"user_tz":-60,"elapsed":240,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"d03fe6a9-de95-4465-da1c-46ddca769c9c"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     model  Template      cors\n","0  BioBERT         0  1.000000\n","1  BioBERT         1  1.000000\n","0  RoBERTa         0  0.998396\n","1  RoBERTa         1  0.998348"],"text/html":["\n","  <div id=\"df-d654e7a5-68d9-4929-befd-930bd9cfb554\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>Template</th>\n","      <th>cors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BioBERT</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BioBERT</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>RoBERTa</td>\n","      <td>0</td>\n","      <td>0.998396</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RoBERTa</td>\n","      <td>1</td>\n","      <td>0.998348</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d654e7a5-68d9-4929-befd-930bd9cfb554')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d654e7a5-68d9-4929-befd-930bd9cfb554 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d654e7a5-68d9-4929-befd-930bd9cfb554');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["#### For task 2:"],"metadata":{"id":"S3v2BgqUGcWM"}},{"cell_type":"code","source":["##  Get JS - Divergences\n","n_sample = 5000\n","positive_pairs = data[data[\"y\"] == 1].reset_index()\n","negative_pairs = data[data[\"y\"] == 0].reset_index()\n","\n","JS_2 = compute_JS_divergences_on_dataset(positive_pairs, negative_pairs, models, new_templates_task2_sentiment, n_sample)\n","JS_2.to_csv(\"prompting-data/results/new_JS_divergence_table_complete_task2_2.tsv\", index=False, sep=\"\\t\")\n","\n","##  Get Top k tokens\n","k = 20\n","TOP_K = compute_top_k_tokens_on_dataset(positive_pairs, negative_pairs, models, new_templates_task2_sentiment, k)\n","TOP_K.to_csv(\"prompting-data/results/new_top_k_table_task2.tsv\", index=False, sep=\"\\t\")\n","\n","TOP_1_COR = compute_top1_correlation(positive_pairs, negative_pairs, models, new_templates_task2_sentiment)\n","TOP_1_COR.to_csv(\"prompting-data/results/new_top_1_cor_task2.tsv\", index=False, sep=\"\\t\")"],"metadata":{"id":"4KSDIkjSGevC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["positive_pairs = data[data[\"y\"] == 1].reset_index()\n","negative_pairs = data[data[\"y\"] == 0].reset_index()\n","\n","\n"],"metadata":{"id":"ihuoVpd7rFOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run expected tokens counts "],"metadata":{"id":"uhkblMWoKENw"}},{"cell_type":"code","source":["save_test_1 = JS_2"],"metadata":{"id":"HSIiS5pXI8si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_test_1.shape"],"metadata":{"id":"etNnEABiJYZn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675089978452,"user_tz":-60,"elapsed":427,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"46cc39fd-a34c-4690-cc21-aece653daa3b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(420000, 5)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["TOP_K.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHA2Tkf8LSTn","executionInfo":{"status":"ok","timestamp":1675079240721,"user_tz":-60,"elapsed":5,"user":{"displayName":"Maxime Delmas","userId":"07247506343550606887"}},"outputId":"9d68a78d-f7e1-4142-8fab-f65eaa18f484"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3360, 7)"]},"metadata":{},"execution_count":48}]}]}